{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from dataLoader import *\n",
    "from utils import *\n",
    "import yaml\n",
    "import os\n",
    "import json\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representativeness (Evaluation with groundtruths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clear_terms(terms):\n",
    "    clear_terms = []\n",
    "    for term in terms:\n",
    "        term = term.lower()\n",
    "        term = \"_\".join(term.split())\n",
    "        clear_terms.append(term)\n",
    "    return np.array(clear_terms)\n",
    "\n",
    "def get_combined_embed(vocab, ename2eid, eid2embed, glove_vectors,  dim=300):\n",
    "    \n",
    "    term2gv = {}\n",
    "    for i, term in enumerate(vocab):\n",
    "        w2v_vec = eid2embed[ename2eid[term]][0] if term in ename2eid else np.zeros((dim,))\n",
    "        glove_vec = glove_vectors[i]\n",
    "        term2gv[term] = np.concatenate((w2v_vec[:dim], glove_vec[:dim]), axis=0)\n",
    "\n",
    "    return term2gv\n",
    "\n",
    "\n",
    "def cosine(term1_vec, term2_vec):\n",
    "    sim = cosine_similarity([term1_vec], [term2_vec])[0,0]\n",
    "    sim = max(0, min(1, sim))\n",
    "    return sim\n",
    "\n",
    "def representativeness(ground_truths, selected_terms, term2gv):\n",
    "    score = 0\n",
    "    idx = []\n",
    "    for gt in ground_truths:\n",
    "        _score = []\n",
    "        for st in selected_terms:\n",
    "            _score.append(cosine(term2gv[gt], term2gv[st]))\n",
    "        score += np.max(_score)\n",
    "    return score/len(ground_truths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result(config_file, gt_file):\n",
    "\n",
    "    with open(config_file, 'r') as ymlfile:\n",
    "        config = yaml.load(ymlfile, Loader=yaml.FullLoader)\n",
    "    \n",
    "    domain_path = config['dataset']['domain_path']\n",
    "    result_path = os.path.join(config['dataset']['domain_path'], config['dataset']['result_folder'])\n",
    "    \n",
    "    eid2ename, ename2eid = loadEidToEntityMap(domain_path + 'intermediate/entity2id.txt')\n",
    "    eid2DocProb = loadEid2DocFeature(domain_path + 'intermediate/eid2DocProb.txt')\n",
    "    eidDocPair2Prob = loadEidDocPairFeature(domain_path + 'intermediate/eidDocPair2prob.txt')\n",
    "    eid2embed = loadEntityEmbedding(domain_path + 'intermediate/eid2embed.txt', dim=300)[0]\n",
    "    \n",
    "    domain_terms = pd.read_csv(domain_path + 'intermediate/entity2freq.txt', sep='\\t', header=None, keep_default_na=False, quoting=csv.QUOTE_NONE).values[:,0]\n",
    "    \n",
    "    gt_file = \"../data/groundtruths/trending_keywords/\"+gt_file\n",
    "    gt = pd.read_csv(gt_file, header=None, keep_default_na=False, quoting=csv.QUOTE_NONE).values[:,0]\n",
    "    gt = get_clear_terms(gt)\n",
    "    gt = np.unique(gt)\n",
    "    domain_terms = get_clear_terms(domain_terms)\n",
    "    \n",
    "    vocab = np.unique(list(domain_terms) + list(gt))\n",
    "    \n",
    "    glove_vectors = load_embeddings_glove(\"../data/glove.42B.300d.txt\", vocab, phrase_connector='_')\n",
    "    \n",
    "    term2gv = get_combined_embed(vocab, ename2eid, eid2embed, glove_vectors,  dim=300)\n",
    "    \n",
    "    topn = 50\n",
    "    \n",
    "    \n",
    "    for file in os.listdir(result_path):\n",
    "        print(file.split('.')[0])\n",
    "        selected_terms = pd.read_csv(result_path+\"/\"+file, header=None, keep_default_na=False, quoting=csv.QUOTE_NONE).values[:topn,0]\n",
    "        rep = representativeness(gt, selected_terms, term2gv)\n",
    "        print(rep)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI 2000-2009 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = \"configs/ai_2000-2009.yaml\"\n",
    "gt_file = \"2000-2009.txt\"\n",
    "result(config_file, gt_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI 2010-2019 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = \"configs/ai_2010-2019.yaml\"\n",
    "gt_file = \"2010-2019.txt\"\n",
    "result(config_file, gt_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI 2020-2021 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = \"configs/ai_2020-2021.yaml\"\n",
    "gt_file = \"2020-2021.txt\"\n",
    "result(config_file, gt_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keywords trends evaluation (Google Trends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trends score from google trends for all the terms for various time stamps\n",
    "selected_term_trends = None\n",
    "with open('../data/arxiv/cs/ai_sp/selected_terms_trends.txt', 'r') as fin:\n",
    "    selected_term_trends = json.loads(fin.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores_by_year_interval(start_year, end_year, trends):\n",
    "    indices = []\n",
    "    trends = np.array(trends)\n",
    "    for entry in trends:\n",
    "        year = int(str(entry[0]).split('-')[0])\n",
    "        if year >= start_year and year <= end_year:\n",
    "            indices += [True]\n",
    "        else: indices += [False]\n",
    "    return trends[indices, 1].astype(float)\n",
    "\n",
    "def get_time_score_by_method(terms, time_scores, time_index):\n",
    "    score = []\n",
    "    for term in terms:\n",
    "        time_score = np.maximum(time_scores[term],1)\n",
    "        score_ = time_score/np.sum(time_score)\n",
    "        score += [score_[time_index]]\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_scores = dict()\n",
    "for term in selected_term_trends:\n",
    "    \n",
    "    if len(selected_term_trends[term]) == 0:\n",
    "        score2000s = 0\n",
    "        score2010s = 0\n",
    "        score2020s = 0\n",
    "    else:\n",
    "        score2000s = get_scores_by_year_interval(2000, 2009, selected_term_trends[term])\n",
    "        score2010s = get_scores_by_year_interval(2010, 2019, selected_term_trends[term])\n",
    "        score2020s = get_scores_by_year_interval(2020, 2021, selected_term_trends[term])\n",
    "        \n",
    "        score2000s = np.sum(score2000s)/max(np.shape(score2000s)[0], 1)\n",
    "        score2010s = np.sum(score2010s)/max(np.shape(score2010s)[0], 1)\n",
    "        score2020s = np.sum(score2020s)/max(np.shape(score2020s)[0], 1)\n",
    "\n",
    "    time_scores[term] = [score2000s, score2010s, score2020s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result(config_file, time_index):\n",
    "    topn = 50\n",
    "    with open(config_file, 'r') as ymlfile:\n",
    "        config = yaml.load(ymlfile, Loader=yaml.FullLoader)\n",
    "    \n",
    "    result_path = os.path.join(config['dataset']['domain_path'], config['dataset']['result_folder'])\n",
    "    scores = []\n",
    "    for file in os.listdir(result_path):\n",
    "        print(file.split('.')[0])\n",
    "        et = pd.read_csv(result_path+\"/\"+file, header=None, keep_default_na=False, quoting=csv.QUOTE_NONE).values[:topn,0]\n",
    "        score = get_time_score_by_method(et, time_scores, time_index)\n",
    "        scores.append(score)\n",
    "        print(round(np.mean(score), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2000-2009 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = \"configs/ai_2000-2009.yaml\"\n",
    "result(config_file, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2000-2009 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = \"configs/ai_2010-2019.yaml\"\n",
    "result(config_file, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2020-2021 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = \"configs/ai_2020-2021.yaml\"\n",
    "result(config_file, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
